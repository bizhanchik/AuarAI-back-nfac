from fastapi import APIRouter, UploadFile, File, HTTPException, Depends, status
from fastapi.responses import JSONResponse
from typing import Dict, Any, List, Optional, Tuple
import os
import io
from PIL import Image
import json
import base64
import mimetypes
import requests
import re
import time
import uuid
from datetime import datetime
from sqlalchemy.orm import Session
from dotenv import load_dotenv
from openai import OpenAI
import pandas as pd

from ..gcs_uploader import gcs_uploader
from ..firebase_auth import get_current_user_firebase
from .. import models
from ..services.image_compression import ImageCompressionService
from ..database import get_db
import logging

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/body-analysis", tags=["body-analysis"])

# ---------- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ë–†–ï–ù–î–û–í –ò –°–¢–ò–õ–ï–ô ----------
STYLE_PRESETS = {
    "men": {
        "casual": ["Levi's", "Nike", "Adidas", "Uniqlo", "J.Crew", "Patagonia", "Champion", "Tommy Hilfiger"],
        "smart_casual": ["Polo Ralph Lauren", "J.Crew", "Banana Republic", "Hugo Boss", "Calvin Klein", "Tommy Hilfiger", "Lacoste"],
        "office": ["Hugo Boss", "Calvin Klein", "Brooks Brothers", "Banana Republic", "Theory", "Suit Supply"],
        "street": ["Nike", "Adidas", "Supreme", "Champion", "Stussy", "Carhartt", "Vans", "Converse"]
    },
    "women": {
        "casual": ["Levi's", "Nike", "Adidas", "Uniqlo", "J.Crew", "Madewell", "Everlane", "Gap"],
        "smart_casual": ["J.Crew", "Banana Republic", "Ann Taylor", "Madewell", "Everlane", "COS", "& Other Stories"],
        "office": ["Theory", "Ann Taylor", "Banana Republic", "Hugo Boss", "Calvin Klein", "Brooks Brothers"],
        "street": ["Nike", "Adidas", "Champion", "Stussy", "Urban Outfitters", "Vans", "Converse"]
    }
}

BRANDS_AVOID = [
    "Shein", "Romwe", "Floerns", "Zaful", "SheIn", "ROMWE", "FLOERNS", "ZAFUL",
    "Allegra K", "ALLEGRA K", "Milumia", "MILUMIA", "Verdusa", "VERDUSA",
    "SweatyRocks", "SWEATYROCKS", "Simplee", "SIMPLEE", "MakeMeChic", "MAKEMECHIC",
    "Blooming Jelly", "BLOOMING JELLY", "R.Vivimos", "R.VIVIMOS"
]

PRICE_RANGE = {"min": 20, "max": 160}
MIN_RATING = 4.2
MIN_REVIEWS = 500

# ---------- CSV –î–ê–ù–ù–´–ï ----------
CSV_FILE_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "classified.csv")

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö CSV
_clothing_data = None

def load_clothing_data() -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–¥–µ–∂–¥—ã –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    global _clothing_data
    if _clothing_data is None:
        try:
            _clothing_data = pd.read_csv(CSV_FILE_PATH)
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(_clothing_data)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ CSV")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
            _clothing_data = pd.DataFrame()  # –ü—É—Å—Ç–æ–π DataFrame –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    return _clothing_data

# ---------- –£–¢–ò–õ–ò–¢–´ ----------
def b64img(image_bytes: bytes) -> str:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64 –¥–ª—è OpenAI API"""
    return f"data:image/jpeg;base64," + base64.b64encode(image_bytes).decode()

def safe_api_call(func, *args, max_retries: int = 3, **kwargs):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ API —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(max_retries):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            logger.warning(f"‚ö†Ô∏è API –æ—à–∏–±–∫–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
            time.sleep(2 ** attempt)  # —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

# Pydantic models for request/response
from pydantic import BaseModel

class BodyAnalysisResult(BaseModel):
    gender: Optional[str] = None
    body_type: Optional[str] = None
    style_goal: Optional[str] = None
    height_cm: Optional[int] = None
    weight_kg: Optional[int] = None

class IdealFits(BaseModel):
    top_fit: Optional[str] = None
    bottom_fit: Optional[str] = None
    preferred_styles: Optional[List[str]] = None
    avoid_styles: Optional[List[str]] = None
    fit_description: Optional[str] = None

class IdealColors(BaseModel):
    best_colors: Optional[List[str]] = None
    avoid_colors: Optional[List[str]] = None
    color_description: Optional[str] = None

class ClothingItem(BaseModel):
    name: str
    price: float
    image_url: str
    product_url: str
    gender: str
    piece_type: str
    subtype: Optional[str] = None
    fit: Optional[str] = None
    style: Optional[str] = None
    season: Optional[str] = None
    main_color: str
    palette_tags: Optional[str] = None

class ClothingRecommendations(BaseModel):
    tops: Optional[List[ClothingItem]] = None
    bottoms: Optional[List[ClothingItem]] = None
    total_found: Optional[Dict[str, int]] = None

class Metadata(BaseModel):
    recommended_brands: Optional[List[str]] = None
    search_queries: Optional[Dict[str, str]] = None

class BodyAnalysisResponse(BaseModel):
    success: bool
    message: str
    analysis: Optional[BodyAnalysisResult] = None
    ideal_fits: Optional[IdealFits] = None
    ideal_colors: Optional[IdealColors] = None
    clothing_recommendations: Optional[ClothingRecommendations] = None
    metadata: Optional[Metadata] = None
    photo_url: Optional[str] = None

class WardrobeCompatibilityResult(BaseModel):
    compatibility_percentage: float
    matching_items: int
    total_items: int
    recommendations: List[str]
    color_matches: List[str]
    style_matches: List[str]
    missing_essentials: List[str]

class WardrobeCompatibilityResponse(BaseModel):
    success: bool
    message: str
    result: Optional[WardrobeCompatibilityResult] = None

# ---------- –ü–û–ò–°–ö –¢–û–í–ê–†–û–í –í CSV ----------
def ai_select_clothing_from_csv(analysis: Dict, max_items: int = 15) -> Tuple[List[ClothingItem], List[ClothingItem]]:
    """–ò–ò –ø–æ–¥–±–æ—Ä –æ–¥–µ–∂–¥—ã –∏–∑ CSV –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChatGPT API"""
    clothing_data = load_clothing_data()
    
    if clothing_data.empty:
        logger.warning("‚ö†Ô∏è CSV –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        return [], []
    
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ò–ò –∞–Ω–∞–ª–∏–∑–∞
        gender = analysis.get('gender_label', 'unisex')
        body_type = analysis.get('body_type', 'unknown')
        style_goal = analysis.get('style_goal', 'casual')
        best_colors = analysis.get('color_palette', {}).get('best_colors', [])
        avoid_colors = analysis.get('color_palette', {}).get('avoid_colors', [])
        recommended_categories = analysis.get('recommended_categories', {})
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—É –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –æ–±—ä–µ–º–∞
        if gender != 'unisex':
            filtered_data = clothing_data[clothing_data['gender'].str.lower() == gender.lower()]
        else:
            filtered_data = clothing_data
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤)
        sample_size = min(100, len(filtered_data))
        sample_data = filtered_data.sample(n=sample_size) if len(filtered_data) > sample_size else filtered_data
        
        # –°–æ–∑–¥–∞–µ–º JSON –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –ò–ò
        items_for_ai = []
        for idx, row in sample_data.iterrows():
            items_for_ai.append({
                "id": idx,
                "name": row['name'],
                "price": row['price'],
                "gender": row['gender'],
                "piece_type": row['piece_type'],
                "subtype": row.get('subtype', ''),
                "fit": row.get('fit', ''),
                "style": row.get('style', ''),
                "season": row.get('season', ''),
                "main_color": row['main_color'],
                "palette_tags": row.get('palette_tags', '')
            })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è ChatGPT
        ai_prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç–∏–ª—é –∏ –º–æ–¥–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –æ —á–µ–ª–æ–≤–µ–∫–µ –∏ –≤—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –≤–µ—â–∏ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –æ–¥–µ–∂–¥—ã.

–î–∞–Ω–Ω—ã–µ –æ —á–µ–ª–æ–≤–µ–∫–µ:
- –ü–æ–ª: {gender}
- –¢–∏–ø —Ñ–∏–≥—É—Ä—ã: {body_type}
- –°—Ç–∏–ª—å: {style_goal}
- –ü–æ–¥—Ö–æ–¥—è—â–∏–µ —Ü–≤–µ—Ç–∞: {', '.join(best_colors) if best_colors else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'}
- –ò–∑–±–µ–≥–∞—Ç—å —Ü–≤–µ—Ç–∞: {', '.join(avoid_colors) if avoid_colors else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'}
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {recommended_categories}

–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–æ–π –æ–¥–µ–∂–¥—ã:
{json.dumps(items_for_ai, ensure_ascii=False, indent=2)}

–í—ã–±–µ—Ä–∏ –º–∞–∫—Å–∏–º—É–º {max_items//2} —Ç–æ–ø–æ–≤ –∏ {max_items//2} –Ω–∏–∑–∞, –∫–æ—Ç–æ—Ä—ã–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥—è—Ç —ç—Ç–æ–º—É —á–µ–ª–æ–≤–µ–∫—É.
–£—á–∏—Ç—ã–≤–∞–π:
1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø—É —Ñ–∏–≥—É—Ä—ã
2. –¶–≤–µ—Ç–æ–≤—É—é –ø–∞–ª–∏—Ç—Ä—É
3. –°—Ç–∏–ª—å
4. –ö–∞—á–µ—Å—Ç–≤–æ –∏ —Ü–µ–Ω—É
5. –°–æ—á–µ—Ç–∞–µ–º–æ—Å—Ç—å –º–µ–∂–¥—É —Å–æ–±–æ–π

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
  "tops": [—Å–ø–∏—Å–æ–∫ ID —Ç–æ–ø–æ–≤],
  "bottoms": [—Å–ø–∏—Å–æ–∫ ID –Ω–∏–∑–∞],
  "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞"
}}"""
        
        # –ó–∞–ø—Ä–æ—Å –∫ ChatGPT API
        response = openai_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å—Ç–∏–ª—é –∏ –º–æ–¥–µ. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON."},
                {"role": "user", "content": ai_prompt}
            ],
            max_tokens=1000,
            temperature=0.3
        )
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç –ò–ò
        ai_response = response.choices[0].message.content
        logger.info(f"ü§ñ –û—Ç–≤–µ—Ç –ò–ò: {ai_response}")
        
        try:
            ai_selection = json.loads(ai_response)
            selected_top_ids = ai_selection.get('tops', [])
            selected_bottom_ids = ai_selection.get('bottoms', [])
            reasoning = ai_selection.get('reasoning', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
            
            logger.info(f"üß† –ò–ò –≤—ã–±—Ä–∞–ª: {len(selected_top_ids)} —Ç–æ–ø–æ–≤, {len(selected_bottom_ids)} –Ω–∏–∑–∞. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {reasoning}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ ID –≤ ClothingItem –æ–±—ä–µ–∫—Ç—ã
            tops_result = []
            for item_id in selected_top_ids:
                if item_id in sample_data.index:
                    row = sample_data.loc[item_id]
                    tops_result.append(ClothingItem(
                        name=row['name'],
                        price=float(row['price']),
                        image_url=row['image_url'],
                        product_url=row['product_url'],
                        gender=row['gender'],
                        piece_type=row['piece_type'],
                        subtype=row.get('subtype'),
                        fit=row.get('fit'),
                        style=row.get('style'),
                        season=row.get('season'),
                        main_color=row['main_color'],
                        palette_tags=row.get('palette_tags')
                    ))
            
            bottoms_result = []
            for item_id in selected_bottom_ids:
                if item_id in sample_data.index:
                    row = sample_data.loc[item_id]
                    bottoms_result.append(ClothingItem(
                        name=row['name'],
                        price=float(row['price']),
                        image_url=row['image_url'],
                        product_url=row['product_url'],
                        gender=row['gender'],
                        piece_type=row['piece_type'],
                        subtype=row.get('subtype'),
                        fit=row.get('fit'),
                        style=row.get('style'),
                        season=row.get('season'),
                        main_color=row['main_color'],
                        palette_tags=row.get('palette_tags')
                    ))
            
            logger.info(f"‚úÖ –ò–ò –ø–æ–¥–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω: {len(tops_result)} —Ç–æ–ø–æ–≤, {len(bottoms_result)} –Ω–∏–∑–∞")
            return tops_result, bottoms_result
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –ò–ò: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
            return fallback_clothing_search(analysis, max_items)
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ò–ò –ø–æ–¥–±–æ—Ä–∞: {e}")
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
        return fallback_clothing_search(analysis, max_items)

def fallback_clothing_search(analysis: Dict, max_items: int = 15) -> Tuple[List[ClothingItem], List[ClothingItem]]:
    """–†–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –æ–¥–µ–∂–¥—ã –≤ —Å–ª—É—á–∞–µ —Å–±–æ—è –ò–ò"""
    clothing_data = load_clothing_data()
    
    if clothing_data.empty:
        return [], []
    
    gender = analysis.get('gender_label', 'unisex')
    recommended_categories = analysis.get('recommended_categories', {})
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª—É
    if gender != 'unisex':
        filtered_data = clothing_data[clothing_data['gender'].str.lower() == gender.lower()]
    else:
        filtered_data = clothing_data
    
    # –ü–æ–∏—Å–∫ —Ç–æ–ø–æ–≤ –∏ –Ω–∏–∑–∞
    top_categories = recommended_categories.get('top', ['t-shirt', 'shirt', 'blouse'])
    bottom_categories = recommended_categories.get('bottom', ['jeans', 'trousers', 'pants'])
    
    tops_data = filtered_data[filtered_data['piece_type'].str.lower().isin([cat.lower() for cat in top_categories])]
    bottoms_data = filtered_data[filtered_data['piece_type'].str.lower().isin([cat.lower() for cat in bottom_categories])]
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ü–µ–Ω–µ
    tops_data = tops_data.sort_values('price', ascending=True)
    bottoms_data = bottoms_data.sort_values('price', ascending=True)
    
    max_per_category = max_items // 2
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ ClothingItem –æ–±—ä–µ–∫—Ç—ã
    tops_result = []
    for _, row in tops_data.head(max_per_category).iterrows():
        tops_result.append(ClothingItem(
            name=row['name'],
            price=float(row['price']),
            image_url=row['image_url'],
            product_url=row['product_url'],
            gender=row['gender'],
            piece_type=row['piece_type'],
            subtype=row.get('subtype'),
            fit=row.get('fit'),
            style=row.get('style'),
            season=row.get('season'),
            main_color=row['main_color'],
            palette_tags=row.get('palette_tags')
        ))
    
    bottoms_result = []
    for _, row in bottoms_data.head(max_per_category).iterrows():
        bottoms_result.append(ClothingItem(
            name=row['name'],
            price=float(row['price']),
            image_url=row['image_url'],
            product_url=row['product_url'],
            gender=row['gender'],
            piece_type=row['piece_type'],
            subtype=row.get('subtype'),
            fit=row.get('fit'),
            style=row.get('style'),
            season=row.get('season'),
            main_color=row['main_color'],
            palette_tags=row.get('palette_tags')
        ))
    
    logger.info(f"üîÑ Fallback –ø–æ–∏—Å–∫: {len(tops_result)} —Ç–æ–ø–æ–≤, {len(bottoms_result)} –Ω–∏–∑–∞")
    return tops_result, bottoms_result

# ---------- –°–¢–ê–†–´–ï –§–£–ù–ö–¶–ò–ò AMAZON (–ë–£–î–£–¢ –ó–ê–ú–ï–ù–ï–ù–´) ----------
def tavily_search_with_brands(query: str, brands: List[str], max_results: int = 12) -> List[str]:
    """–ü–æ–∏—Å–∫ –Ω–∞ Amazon —á–µ—Ä–µ–∑ Tavily —Å —É—á–µ—Ç–æ–º –±—Ä–µ–Ω–¥–æ–≤"""
    # –î–æ–±–∞–≤–ª—è–µ–º –±—Ä–µ–Ω–¥—ã –≤ –∑–∞–ø—Ä–æ—Å
    brand_queries = []
    for brand in brands[:3]:  # –±–µ—Ä–µ–º —Ç–æ–ø-3 –±—Ä–µ–Ω–¥–∞
        brand_query = f'"{brand}" {query} dp site:amazon.com'
        brand_queries.append(brand_query)
    
    all_urls = []
    for bq in brand_queries:
        try:
            r = safe_api_call(
                requests.post,
                "https://api.tavily.com/search",
                json={
                    "api_key": TAVILY_API_KEY,
                    "query": bq,
                    "search_depth": "basic",
                    "include_domains": ["amazon.com"],
                    "max_results": max_results // len(brand_queries) + 2,
                },
                timeout=25,
            )
            r.raise_for_status()
            data = r.json()
            urls = [it.get("url", "") for it in data.get("results", []) if isinstance(it, dict)]
            all_urls.extend(urls)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –±—Ä–µ–Ω–¥–∞ {bq}: {e}")
            continue
    
    return filter_amazon_links(all_urls, max_results)

def filter_amazon_links(urls: List[str], max_results: int) -> List[str]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ Amazon –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞"""
    cleaned, seen = [], set()
    
    for url in urls:
        if not isinstance(url, str) or url in seen:
            continue
            
        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        clean_url = url.split("?")[0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ Amazon
        if "amazon.com" not in clean_url:
            continue
            
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
        if not ("/dp/" in clean_url or "/gp/" in clean_url):
            continue
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤ –≤ URL
        url_lower = clean_url.lower()
        if any(brand.lower() in url_lower for brand in BRANDS_AVOID):
            continue
            
        if clean_url not in seen:
            cleaned.append(clean_url)
            seen.add(clean_url)
            
        if len(cleaned) >= max_results:
            break
    
    return cleaned

# ---------- –ê–ù–ê–õ–ò–ó –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ----------
def analyze_image(image_bytes: bytes) -> Dict:
    """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–¥–µ–∂–¥—ã"""
    img = b64img(image_bytes)

    # JSON Schema –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    schema = {
        "type": "object",
        "additionalProperties": False,
        "properties": {
            "gender_label": {"type": "string", "enum": ["men", "women", "unisex"]},
            "height_cm": {"type": ["integer", "number"]},
            "weight_kg": {"type": ["integer", "number"]},
            "body_type": {
                "type": "string",
                "enum": ["rectangle", "inverted_triangle", "triangle", "hourglass", "oval"]
            },
            "shoulder_hip_ratio": {"type": "string", "enum": ["low", "balanced", "high"]},
            "vertical_balance": {"type": "string", "enum": ["short_legged", "balanced", "long_legged"]},
            "style_goal": {"type": "string", "enum": ["casual", "smart_casual", "office", "street"]},
            "fit_rules": {
                "type": "object",
                "additionalProperties": False,
                "properties": {
                    "top_fit": {"type": "string"},
                    "bottom_fit": {"type": "string"},
                    "avoid": {"type": "array", "items": {"type": "string"}},
                    "prefer": {"type": "array", "items": {"type": "string"}}
                },
                "required": ["top_fit", "bottom_fit", "avoid", "prefer"]
            },
            "color_palette": {
                "type": "object",
                "additionalProperties": False,
                "properties": {
                    "season": {
                        "type": "string",
                        "enum": ["cool_summer", "cool_winter", "warm_spring", "warm_autumn", "neutral"]
                    },
                    "best_colors": {"type": "array", "items": {"type": "string"}},
                    "avoid_colors": {"type": "array", "items": {"type": "string"}}
                },
                "required": ["season", "best_colors", "avoid_colors"]
            },
            "recommended_categories": {
                "type": "object",
                "additionalProperties": False,
                "properties": {
                    "top": {
                        "type": "array",
                        "minItems": 1,
                        "maxItems": 3,
                        "items": {"type": "string"}
                    },
                    "bottom": {
                        "type": "array",
                        "minItems": 1,
                        "maxItems": 3,
                        "items": {"type": "string"}
                    }
                },
                "required": ["top", "bottom"]
            },
            "size_hints": {"type": "string"}
        },
        "required": [
            "gender_label", "height_cm", "weight_kg", "body_type", "shoulder_hip_ratio", "vertical_balance",
            "style_goal", "fit_rules", "color_palette", "recommended_categories", "size_hints"
        ]
    }

    # –í—ã–∑–æ–≤ OpenAI API —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    resp = safe_api_call(
        openai_client.chat.completions.create,
        model="gpt-4.1-mini",
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "StyleAnalysis",
                "schema": schema,
                "strict": True
            }
        },
        messages=[{
            "role": "user",
            "content": [
                {"type": "text", "text":
                    "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–æ—Ç–æ –∏ –≤–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON –ø–æ —Å—Ö–µ–º–µ. "
                    "–û–ø—Ä–µ–¥–µ–ª–∏ style_goal (casual/smart_casual/office/street) –∏—Å—Ö–æ–¥—è –∏–∑ –≤–Ω–µ—à–Ω–æ—Å—Ç–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. "
                    "–ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ —Ä–æ—Å—Ç–µ/–≤–µ—Å–µ ‚Äî –æ—Ü–µ–Ω–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ. "
                    "–í fit_rules —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞—Å–æ–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–ª–æ—Å–ª–æ–∂–µ–Ω–∏—è. "
                    "–ü–æ–¥–±–µ—Ä–∏ —Ç–æ—á–Ω—É—é —Ü–≤–µ—Ç–æ–≤—É—é –ø–∞–ª–∏—Ç—Ä—É –ø–æ —Ü–≤–µ—Ç–æ—Ç–∏–ø—É (—Ç–µ–ø–ª—ã–π/—Ö–æ–ª–æ–¥–Ω—ã–π, –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å)."
                },
                {"type": "image_url", "image_url": {"url": img}}
            ]
        }]
    )

    data = json.loads(resp.choices[0].message.content)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–µ—Ñ–æ–ª—Ç—ã
    data["gender_label"] = (data.get("gender_label") or "unisex").lower()
    if data["gender_label"] not in ("men", "women", "unisex"):
        data["gender_label"] = "unisex"
    
    # –î–µ—Ñ–æ–ª—Ç –¥–ª—è style_goal
    if not data.get("style_goal") or data["style_goal"] not in ["casual", "smart_casual", "office", "street"]:
        data["style_goal"] = "casual"

    # –°—Ç—Ä–∞—Ö—É–µ–º –ø—É—Å—Ç—ã–µ –º–∞—Å—Å–∏–≤—ã
    rc = data.get("recommended_categories", {}) or {}
    if not rc.get("top"): rc["top"] = ["t-shirt"]
    if not rc.get("bottom"): rc["bottom"] = ["jeans"]
    data["recommended_categories"] = rc

    cp = data.get("color_palette", {}) or {}
    if not cp.get("best_colors"): cp["best_colors"] = ["white", "black", "navy"]
    if not cp.get("avoid_colors"): cp["avoid_colors"] = []
    data["color_palette"] = cp

    fr = data.get("fit_rules", {}) or {}
    fr.setdefault("top_fit", "regular")
    fr.setdefault("bottom_fit", "straight")
    fr.setdefault("avoid", [])
    fr.setdefault("prefer", [])
    data["fit_rules"] = fr

    return data

# ---------- –ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–û–ò–°–ö–û–í–´–• –ó–ê–ü–†–û–°–û–í ----------
def build_queries_from_analysis(analysis: Dict) -> Tuple[str, str, List[str]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ —Å–ø–∏—Å–æ–∫ –±—Ä–µ–Ω–¥–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞"""
    gender = analysis.get("gender_label", "unisex")
    style_goal = analysis.get("style_goal", "casual")
    
    # –ü–æ–ª—É—á–∞–µ–º –±—Ä–µ–Ω–¥—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–æ–ª–∞ –∏ —Å—Ç–∏–ª—è
    brands = []
    if gender in STYLE_PRESETS and style_goal in STYLE_PRESETS[gender]:
        brands = STYLE_PRESETS[gender][style_goal]
    elif gender in STYLE_PRESETS and "casual" in STYLE_PRESETS[gender]:
        brands = STYLE_PRESETS[gender]["casual"]  # —Ñ–æ–ª–±—ç–∫ –Ω–∞ casual
    else:
        brands = ["Uniqlo", "J.Crew", "Nike"]  # —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ñ–æ–ª–±—ç–∫
    
    top_cat = (analysis["recommended_categories"]["top"][0]).lower()
    bottom_cat = (analysis["recommended_categories"]["bottom"][0]).lower()

    top_fit = analysis["fit_rules"]["top_fit"]
    bottom_fit = analysis["fit_rules"]["bottom_fit"]

    best_colors = analysis["color_palette"]["best_colors"][:2]
    color_part = " ".join(best_colors).strip()

    def build_query(category: str, fit: str) -> str:
        parts = [gender, category, fit, color_part]
        return " ".join(p for p in parts if p).replace("  ", " ").strip()

    top_query = build_query(top_cat, top_fit)
    bottom_query = build_query(bottom_cat, bottom_fit)
    
    return top_query, bottom_query, brands

# ---------- –§–ò–ù–ê–õ–¨–ù–´–ô –í–´–ë–û–† ----------
def select_best_items(analysis: Dict, top_links: List[str], bottom_links: List[str], max_items: int = 15) -> Tuple[List[str], List[str]]:
    """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤–µ—Ä—Ö–∞ –∏ –Ω–∏–∑–∞ –∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞"""
    if not top_links or not bottom_links:
        return [], []
    
    # –ü–æ–ª—É—á–∞–µ–º –±—Ä–µ–Ω–¥—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    gender = analysis.get("gender_label", "unisex")
    style_goal = analysis.get("style_goal", "casual")
    preferred_brands = []
    if gender in STYLE_PRESETS and style_goal in STYLE_PRESETS[gender]:
        preferred_brands = STYLE_PRESETS[gender][style_goal]
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ max_items
    selected_tops = top_links[:max_items]
    selected_bottoms = bottom_links[:max_items]
    
    # –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –±—Ä–µ–Ω–¥–∞–º (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º—ã–º)
    def sort_by_brand_preference(links: List[str]) -> List[str]:
        preferred = []
        others = []
        
        for link in links:
            link_lower = link.lower()
            is_preferred = any(brand.lower() in link_lower for brand in preferred_brands)
            if is_preferred:
                preferred.append(link)
            else:
                others.append(link)
        
        return preferred + others
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º –±—Ä–µ–Ω–¥–æ–≤
    if preferred_brands:
        selected_tops = sort_by_brand_preference(selected_tops)
        selected_bottoms = sort_by_brand_preference(selected_bottoms)
    
    return selected_tops, selected_bottoms
@router.post("/wardrobe-compatibility", response_model=WardrobeCompatibilityResponse)
async def analyze_wardrobe_compatibility(
    current_user: models.User = Depends(get_current_user_firebase),
    db: Session = Depends(get_db)
):
    """
    Analyze how well user's wardrobe matches their body analysis results.
    """
    try:
        logger.info(f"üîç Starting wardrobe compatibility analysis for user {current_user.id}")
        
        # Get user's clothing items
        clothing_items = db.query(models.ClothingItem).filter(
            models.ClothingItem.owner_id == current_user.id
        ).all()
        
        if not clothing_items:
            return WardrobeCompatibilityResponse(
                success=True,
                message="No clothing items found in wardrobe",
                result=WardrobeCompatibilityResult(
                    compatibility_percentage=0.0,
                    matching_items=0,
                    total_items=0,
                    recommendations=["Add clothing items to your wardrobe to get compatibility analysis"],
                    color_matches=[],
                    style_matches=[],
                    missing_essentials=["Basic wardrobe items needed"]
                )
            )
        
        # Get user's latest body analysis (we'll need to store this)
        # For now, we'll use default recommendations
        body_analysis = {
            "bodyType": "Rectangle",  # Default - should come from stored analysis
            "recommendedColors": ["Navy", "White", "Black", "Gray", "Burgundy"],
            "styleRecommendations": ["Classic", "Minimalist", "Structured"]
        }
        
        # Prepare wardrobe data for AI analysis
        wardrobe_data = []
        for item in clothing_items:
            wardrobe_data.append({
                "name": item.name,
                "category": item.category,
                "color": item.color,
                "brand": item.brand,
                "material": item.material,
                "tags": item.tags if hasattr(item, 'tags') else []
            })
        
        # Analyze compatibility with AI
        try:
            logger.info("ü§ñ Starting AI wardrobe compatibility analysis...")
            compatibility_result = ai_analyze_wardrobe_compatibility(body_analysis, wardrobe_data)
            logger.info(f"‚úÖ AI compatibility analysis completed: {compatibility_result}")
            
        except Exception as e:
            logger.error(f"‚ùå AI compatibility analysis failed: {e}")
            # Fallback to basic analysis
            compatibility_result = {
                "compatibility_percentage": 75.0,
                "matching_items": len(clothing_items) // 2,
                "total_items": len(clothing_items),
                "recommendations": ["Consider adding more versatile pieces", "Focus on recommended colors"],
                "color_matches": ["Navy", "Black"],
                "style_matches": ["Classic pieces"],
                "missing_essentials": ["White button-down shirt", "Dark jeans"]
            }
        
        result = WardrobeCompatibilityResult(
            compatibility_percentage=compatibility_result.get("compatibility_percentage", 0.0),
            matching_items=compatibility_result.get("matching_items", 0),
            total_items=len(clothing_items),
            recommendations=compatibility_result.get("recommendations", []),
            color_matches=compatibility_result.get("color_matches", []),
            style_matches=compatibility_result.get("style_matches", []),
            missing_essentials=compatibility_result.get("missing_essentials", [])
        )
        
        return WardrobeCompatibilityResponse(
            success=True,
            message="Wardrobe compatibility analysis completed",
            result=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in wardrobe compatibility analysis: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during wardrobe compatibility analysis"
        )

@router.post("/analyze", response_model=BodyAnalysisResponse)
async def analyze_body_photo(
    file: UploadFile = File(...),
    current_user: models.User = Depends(get_current_user_firebase)
) -> BodyAnalysisResponse:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–æ—Ç–æ —Ç–µ–ª–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç–∏–ª—é —Å –ø–æ–∏—Å–∫–æ–º —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ Amazon
    """
    try:
        logger.info(f"üîç Starting body photo analysis for user {current_user.firebase_uid or 'unknown'}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–¥–µ–∂–¥—ã –∏–∑ CSV
        load_clothing_data()
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
        if not file.content_type or not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        image_bytes = await file.read()
        if len(image_bytes) == 0:
            raise HTTPException(status_code=400, detail="–§–∞–π–ª –ø—É—Å—Ç–æ–π")
        
        logger.info(f"üì∏ Processing image: {len(image_bytes)} bytes")
        
        # –°–∂–∞—Ç–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        try:
            storage_compressed = ImageCompressionService.compress_for_storage(image_bytes)
            logger.info("üì¶ Image compressed for storage")
        except Exception as e:
            logger.error(f"‚ùå Image compression failed: {e}")
            raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ GCS
        try:
            filename = f"body_analysis/{current_user.firebase_uid or 'unknown'}/{uuid.uuid4()}.jpg"
            public_url = gcs_uploader.upload_file(
                file_data=storage_compressed,
                filename=filename,
                content_type="image/jpeg"
            )
            logger.info(f"‚òÅÔ∏è Image uploaded to GCS: {public_url}")
        except Exception as e:
            logger.error(f"‚ùå GCS upload failed: {e}")
            raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞")
        
        # AI –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OpenAI GPT-4o
        try:
            logger.info("ü§ñ Starting AI body analysis...")
            analysis = analyze_image(image_bytes)
            logger.info(f"‚úÖ AI analysis completed")
        except Exception as e:
            logger.error(f"‚ùå AI analysis failed: {e}")
            raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        try:
            top_query, bottom_query, preferred_brands = build_queries_from_analysis(analysis)
            logger.info(f"üîç Generated queries - Top: {top_query}, Bottom: {bottom_query}")
        except Exception as e:
            logger.error(f"‚ùå Query generation failed: {e}")
            top_query, bottom_query, preferred_brands = "casual shirt", "jeans", ["Uniqlo", "J.Crew"]
        
        # –ò–ò –ø–æ–¥–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ CSV –¥–∞–Ω–Ω—ã—Ö
        try:
            logger.info("ü§ñ AI selecting clothing from CSV data...")
            final_tops, final_bottoms = ai_select_clothing_from_csv(analysis, max_items=15)
            
            logger.info(f"üéØ AI selected {len(final_tops)} tops and {len(final_bottoms)} bottoms")
        except Exception as e:
            logger.error(f"‚ùå CSV product search failed: {e}")
            final_tops, final_bottoms = [], []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        result = BodyAnalysisResult(
            gender=analysis.get("gender_label", "unisex"),
            body_type=analysis.get("body_type", "rectangle"),
            style_goal=analysis.get("style_goal", "casual"),
            height_cm=analysis.get("height_cm", 170),
            weight_kg=analysis.get("weight_kg", 70)
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω—ã—Ö —Ñ–∞—Å–æ–Ω–æ–≤ –∏ —Ü–≤–µ—Ç–æ–≤
        fits = IdealFits(
            top_fit=analysis["fit_rules"]["top_fit"],
            bottom_fit=analysis["fit_rules"]["bottom_fit"],
            preferred_styles=analysis["fit_rules"]["prefer"],
            avoid_styles=analysis["fit_rules"]["avoid"],
            fit_description=f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∫—Ä–æ–π –≤–µ—Ä—Ö–∞: {analysis['fit_rules']['top_fit']}, –Ω–∏–∑–∞: {analysis['fit_rules']['bottom_fit']}"
        )
        
        colors = IdealColors(
            best_colors=analysis["color_palette"]["best_colors"],
            avoid_colors=analysis["color_palette"]["avoid_colors"],
            color_description=f"–¶–≤–µ—Ç–æ—Ç–∏–ø: {analysis['color_palette']['season']}"
        )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ–¥–µ–∂–¥—ã –∏–∑ CSV
        clothing_recs = ClothingRecommendations(
            tops=final_tops,
            bottoms=final_bottoms,
            total_found={
                "tops": len(final_tops),
                "bottoms": len(final_bottoms)
            }
        )
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = Metadata(
            recommended_brands=preferred_brands,
            search_queries={
                "top_query": top_query,
                "bottom_query": bottom_query
            }
        )
        
        return BodyAnalysisResponse(
            success=True,
            message="–ê–Ω–∞–ª–∏–∑ —Ç–µ–ª–∞ –∏ –ø–æ–¥–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω",
            analysis=result,
            ideal_fits=fits,
            ideal_colors=colors,
            clothing_recommendations=clothing_recs,
            metadata=metadata,
            photo_url=public_url
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in body analysis: {e}")
        raise HTTPException(status_code=500, detail="–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞")

@router.post("/wardrobe-compatibility", response_model=WardrobeCompatibilityResponse)
async def analyze_wardrobe_compatibility(
    current_user: models.User = Depends(get_current_user_firebase),
    db: Session = Depends(get_db)
):
    """
    Analyze how well user's wardrobe matches their body analysis results.
    """
    try:
        logger.info(f"üîç Starting wardrobe compatibility analysis for user {current_user.id}")
        
        # Get user's clothing items
        clothing_items = db.query(models.ClothingItem).filter(
            models.ClothingItem.owner_id == current_user.id
        ).all()
        
        if not clothing_items:
            return WardrobeCompatibilityResponse(
                success=True,
                message="No clothing items found in wardrobe",
                result=WardrobeCompatibilityResult(
                    compatibility_percentage=0.0,
                    matching_items=0,
                    total_items=0,
                    recommendations=["Add clothing items to your wardrobe to get compatibility analysis"],
                    color_matches=[],
                    style_matches=[],
                    missing_essentials=["Basic wardrobe items needed"]
                )
            )
        
        # Get user's latest body analysis (we'll need to store this)
        # For now, we'll use default recommendations
        body_analysis = {
            "bodyType": "Rectangle",  # Default - should come from stored analysis
            "recommendedColors": ["Navy", "White", "Black", "Gray", "Burgundy"],
            "styleRecommendations": ["Classic", "Minimalist", "Structured"]
        }
        
        # Prepare wardrobe data for AI analysis
        wardrobe_data = []
        for item in clothing_items:
            wardrobe_data.append({
                "name": item.name,
                "category": item.category,
                "color": item.color,
                "brand": item.brand,
                "material": item.material,
                "tags": item.tags if hasattr(item, 'tags') else []
            })
        
        # Analyze compatibility with AI
        try:
            logger.info("ü§ñ Starting AI wardrobe compatibility analysis...")
            compatibility_result = ai_analyze_wardrobe_compatibility(body_analysis, wardrobe_data)
            logger.info(f"‚úÖ AI compatibility analysis completed: {compatibility_result}")
            
        except Exception as e:
            logger.error(f"‚ùå AI compatibility analysis failed: {e}")
            # Fallback to basic analysis
            compatibility_result = {
                "compatibility_percentage": 75.0,
                "matching_items": len(clothing_items) // 2,
                "total_items": len(clothing_items),
                "recommendations": ["Consider adding more versatile pieces", "Focus on recommended colors"],
                "color_matches": ["Navy", "Black"],
                "style_matches": ["Classic pieces"],
                "missing_essentials": ["White button-down shirt", "Dark jeans"]
            }
        
        result = WardrobeCompatibilityResult(
            compatibility_percentage=compatibility_result.get("compatibility_percentage", 0.0),
            matching_items=compatibility_result.get("matching_items", 0),
            total_items=len(clothing_items),
            recommendations=compatibility_result.get("recommendations", []),
            color_matches=compatibility_result.get("color_matches", []),
            style_matches=compatibility_result.get("style_matches", []),
            missing_essentials=compatibility_result.get("missing_essentials", [])
        )
        
        return WardrobeCompatibilityResponse(
            success=True,
            message="Wardrobe compatibility analysis completed",
            result=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error in wardrobe compatibility analysis: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error during wardrobe compatibility analysis"
        )

@router.get("/results/{user_id}")
async def get_body_analysis_results(
    user_id: int,
    current_user: models.User = Depends(get_current_user_firebase)
):
    """
    Body analysis results are not stored - this endpoint is deprecated.
    """
    raise HTTPException(
        status_code=status.HTTP_404_NOT_FOUND,
        detail="Body analysis results are not stored. Please perform a new analysis."
    )

@router.delete("/results/{analysis_id}")
async def delete_body_analysis(
    analysis_id: int,
    current_user: models.User = Depends(get_current_user_firebase)
):
    """
    Body analysis results are not stored - this endpoint is deprecated.
    """
    raise HTTPException(
        status_code=status.HTTP_404_NOT_FOUND,
        detail="Body analysis results are not stored. Nothing to delete."
    )